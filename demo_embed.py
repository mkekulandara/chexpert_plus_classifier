# -*- coding: utf-8 -*-
"""demo_embed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S0LKmWly-fW8sGbQZ-IrEBLyAhhYWhZh
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from transformers import BertTokenizer, BertModel
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
import numpy as np
import tensorflow as tf

#read "https://github.com/mkekulandara/chexpert_plus_classifier/blob/main/data/df_chexpert_plus_240401_small.csv" this file from github

import pandas as pd
df = pd.read_csv("https://raw.githubusercontent.com/mkekulandara/chexpert_plus_classifier/main/data/df_chexpert_plus_240401_small.csv")
df.head(2)

print(df['split'].value_counts())

#print colums

print(df.columns)

# prompt: extract 'age', 'sex', 'race',
#        'ethnicity', 'interpreter_needed', 'insurance_type', 'recent_bmi',
#        'deceased' to a new dataframe

new_df = df[['age', 'sex', 'race', 'ethnicity', 'interpreter_needed', 'insurance_type', 'recent_bmi', 'deceased']]
new_df.head(2)

new_df.shape

def convert_patient_info_to_text(row):
    return (f"The patient is {str(row['age'])} years old, identifies as {row['sex']}, and belongs to the {row['race']} race. "
            f"They are of {row['ethnicity']} ethnicity. Interpreter services were{' not' if row['interpreter_needed'] == 'No' else ''} required for this patient. "
            f"The patient has {row['insurance_type']} insurance and a recent BMI of {str(row['recent_bmi'])}. "
            f"At the time of the last record, the patient was{' not' if row['deceased'] == 'No' else ''} deceased.")

# # Example usage:
# for index, row in new_df.iterrows():
#   convert_patient_info_to_text(row)

# Apply the conversion to the whole dataset
new_df['text'] = new_df.apply(convert_patient_info_to_text, axis=1)

new_df['text'].head(3)

# Load pre-trained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Tokenize and encode the text data
inputs = tokenizer(new_df['text'].tolist(), return_tensors='pt', padding=True, truncation=True, max_length=512)

# Extract embeddings from the [CLS] token
with torch.no_grad():
    outputs = model(**inputs)
text_embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # Use the [CLS] token's embedding

# Assuming 'text_embeddings' is a NumPy array
np.save('chex_demo_text_embeddings.npy', text_embeddings)

print(text_embeddings.shape)

text_embeddings[0]